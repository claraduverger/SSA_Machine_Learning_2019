{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is put together by [Zefeng Li](http://web.gps.caltech.edu/~zefengli/) from [Caltech Seismo Lab](http://www.seismolab.caltech.edu/) for the 2019 SSA Machine Learning workshop. If you have any questions, feel free to contact him via zefengli@caltech.edu. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction of ML Regression\n",
    "Supervised ML is basically to find a function f(X)= y that maps your input (X) to your output (y). In classification tasks, which you have seen just now, the prediction target y is categorical. In regression tasks, y is a continuous variable. The basic steps remain largely the same as classification. \n",
    "\n",
    "Similar to classifcation, there are a wide variety of choices for models f (ANN, SVM, Decision Tree, Deep networks etc.); you also have choices for the forms of input X, e.g. the objects themselves or the features of the objects. You will see some difference in evaluatation of prediction accuracy from classifiction. \n",
    "\n",
    "In the following, we will see examples of magnitude estimation using individual early P waveforms. This is an important but challenging problem in earthquake early warning. We will use the same earthquake waveforms which you used in the previous classification example. Prediction of magnitude from P waveforms is not as intuitive as noise/event classification. It appears to be a difficult task (and it is). We see will how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some popular modules that will be useful later\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-poster')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data\n",
    "\n",
    "The first step is always reading in the data into a good format so that we can input into ML algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from obspy import read # read sac/mseed/segy etc.\n",
    "import glob \n",
    "\n",
    "training_folder = '../data/training_data_classification_regression/'\n",
    "waveforms = [] # initilize a empty list to contain data\n",
    "\n",
    "for item in glob.glob(training_folder + 'seismic/*.mseed'):\n",
    "    st = read(item)\n",
    "    st.detrend('demean').detrend('linear')\n",
    "    waveforms.append([st[0].data[100:400], st[1].data[100:400], st[2].data[100:400]]) # 3-s P wave\n",
    "    \n",
    "waveforms = np.asarray(waveforms) # transform list to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveforms.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(training_folder + 'EQs_M3_above.csv') # read meta data of the waveforms as data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['mag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EQ waveform plots\n",
    "Here we plot out the EQ waveforms to make sure the data are reasonable, also to get a sense of the task complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earthquake waveform examples\n",
    "i_record = 100\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.subplot(311)\n",
    "plt.plot(waveforms[i_record, 0, :], 'k')\n",
    "plt.title(\"M = \" + str(y[i_record]))\n",
    "plt.subplot(312)\n",
    "plt.plot(waveforms[i_record, 1, :], 'r')\n",
    "plt.subplot(313)\n",
    "plt.plot(waveforms[i_record, 2, :], 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Do you see any problem from the above examples?\n",
    "Unbalanced data is an annoying problem in many ML tasks. We will not try to solve it in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: Get the input (object features vs. objects themselves)\n",
    "There are basically two types of inputs: a. the features of the object; b. the objects themselves. Let's see the first case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features\n",
    "Here we calculate only 4 features (of course, you can calculate more using tsfresh https://tsfresh.readthedocs.io/): mean value, max value, rms (the same as std), and number of zero crossings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(data):\n",
    "    \n",
    "    mean_amp = np.apply_along_axis(np.mean, 1, np.abs(data))\n",
    "    max_amp = np.apply_along_axis(max, 1, np.abs(data))\n",
    "    std_amp = np.apply_along_axis(np.std, 1, data)\n",
    "    zero_crossings = lambda data: len(np.where(np.diff(np.sign(data)))[0])\n",
    "    zc = np.apply_along_axis(zero_crossings, 1, data)\n",
    "    \n",
    "    return np.log10(max(mean_amp)), np.log10(max(max_amp)), np.log10(max(std_amp)), max(zc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate all the features for each record\n",
    "X = np.array(list(map(calculate_features, waveforms)))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['mean_amp', 'max_amp', 'std_amp', 'zc']\n",
    "fig, axs = plt.subplots(1, 4, sharey = True, figsize = (16, 8))\n",
    "for i, c in zip([0, 1, 2, 3], ['k', 'r', 'g', 'b']):\n",
    "    axs[i].plot(X[:, i], y, 'o', color = c)\n",
    "    axs[i].set_xlabel(feature_names[i])\n",
    "axs[0].set_ylabel('Mag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide training/testing dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standard_scaler = StandardScaler()\n",
    "X_train_scale = standard_scaler.fit_transform(X_train)\n",
    "print(X_train_scale.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, sharey = True, figsize = (15, 8))\n",
    "for i, c in zip([0, 1, 2, 3], ['k', 'r', 'g', 'b']):\n",
    "    axs[i].plot(X_train_scale[:, i], y_train, 'o', color = c)\n",
    "    axs[i].set_xlim([-3, 3])\n",
    "    axs[i].set_xlabel(feature_names[i])\n",
    "axs[0].set_ylabel('mag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "![Desion Tree](https://cdn-images-1.medium.com/max/1600/1*H3nZElqhfOE35AFAq8gy0A.png)\n",
    "## Random Forest\n",
    "![Random Forest](https://cdn-images-1.medium.com/max/1600/1*i0o8mjFfCn-uD79-F1Cqkw.png)\n",
    "\n",
    "Courtesy: Pictures from https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all the features, scale input\n",
    "X_train_scale = standard_scaler.fit_transform(X_train)\n",
    "X_test_scale = standard_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "n_estimators = [10, 20, 40, 80] # stabilize the estimator but increase computational cost\n",
    "max_depth =[5, 10, 20, 40] # overfit if too high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# Tuning hyper-parameters:\")\n",
    "for i_estimators in [10, 20, 50, 100]:\n",
    "    for i_max_depth in [10, 20, 50]:\n",
    "        regr = RandomForestRegressor(n_estimators=i_estimators, \n",
    "                                    max_depth=i_max_depth, \n",
    "                                    random_state=0, \n",
    "                                    criterion='mse')\n",
    "        regr.fit(X_train_scale, y_train) # train\n",
    "        print('n_estimators = {}, max_depth = {}'.format(i_estimators, i_max_depth))\n",
    "        # Score: R^2 bt predicted and true values, at best 1. It can be negative.\n",
    "        print('score on test set: {}'.format(regr.score(X_test_scale, y_test))) \n",
    "        print('feature importances: {}'.format(regr.feature_importances_))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the RF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a specific setting to show the results\n",
    "regr = RandomForestRegressor(n_estimators=50, \n",
    "                             max_depth=50, \n",
    "                             random_state=0, \n",
    "                             criterion='mse')\n",
    "regr.fit(X_train_scale, y_train)\n",
    "y_pred = regr.predict(X_test_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE:', np.std(y_pred-y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 1-1 comparison\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.plot(y_test, y_pred, 'o')\n",
    "plt.axis('square')\n",
    "plt.xlim([2.5, 4.5])\n",
    "plt.ylim([2.5, 4.5])\n",
    "plt.xlabel('y_test')\n",
    "plt.ylabel('y_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residual histogram\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.hist(y_pred-y_test)\n",
    "plt.xlabel('y residual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A deep learning model with direct waveform input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = waveforms\n",
    "for i in range(X.shape[0]):\n",
    "    X[i] = X[i]/np.max(np.std(X[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:, :, :, np.newaxis] # This treat 1D time series as 2D images by adding one more dimension\n",
    "input_shape = (X.shape[1], X.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state = 7) # Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from the classifcation example\n",
    "inputs = Input(input_shape)\n",
    "x = Conv2D(8, (1, 3), activation='relu')(inputs)\n",
    "x = MaxPooling2D((1, 2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "out = Dense(1, activation='linear')(x) # Last output should be a scalar for regression\n",
    "model = Model(inputs = inputs, outputs = out)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 20\n",
    "\n",
    "model.compile(loss=keras.losses.mean_squared_error,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['mse']) # metrics MSE for regression\n",
    "\n",
    "# use 20% of the training data as validation\n",
    "history = model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training procedure\n",
    "mse = history.history['loss']\n",
    "val_mse = history.history['val_loss']\n",
    "epochs_axis = range(1, len(mse) + 1)\n",
    "plt.plot(epochs_axis, mse, 'b', label='Training accuracy')\n",
    "plt.plot(epochs_axis, val_mse, 'r', label='Validation accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "plt.plot(y_test, y_pred, 'o')\n",
    "plt.axis('square')\n",
    "plt.xlabel('y_test')\n",
    "plt.ylabel('y_pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflect on why so poor performance\n",
    "1. The task by itself is difficult. (Imagine to reason the magnitude by looking at a P wave)\n",
    "2. Data set is too unbalanced (Few in large magnitude)\n",
    "3. Data set is too small (only 256 events)\n",
    "4. Too simple models (only 4 features for Random Forest, only a few layers in the NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example](http://web.gps.caltech.edu/~zefengli/transfer/Slides/Example.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
